{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfM3iBlLlhVo",
        "tags": []
      },
      "source": [
        "# ID2214/FID3214 Assignment 3 Group no. ID2214 â€“ 5\n",
        "### Project members:\n",
        "Lukas Olenborg,\n",
        "\n",
        "Charlotte Jacquet,\n",
        "\n",
        "Isabella Rositi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pvGZrxb238p",
        "outputId": "1fc781aa-ab57-4df0-e4b7-500349a5a39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZtML7sM3Cav"
      },
      "outputs": [],
      "source": [
        "# Filepath to shared drive (you have to add the 'Programming for Data Science' folder to your drive)\n",
        "#filepath = '/content/drive/MyDrive/Programming for Data Science/Assignment 3/'\n",
        "\n",
        "# Filepath to your own drive folder and files if above doesnt work\n",
        "#filepath = '/content/drive/MyDrive/Colab Notebooks/ID2214/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PSGY6vq3EmH"
      },
      "outputs": [],
      "source": [
        "# Filepath for TA, uncomment when submitting\n",
        "filepath = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bokXHfOTlhVt"
      },
      "source": [
        "## Load NumPy, pandas, time and DecisionTreeClassifier from sklearn.tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUYafFjwlhVu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxfvpjvjlhVv",
        "outputId": "5818f383-060b-4af8-ce3d-9092fe33b092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.7.15\n",
            "NumPy version: 1.21.6\n",
            "Pandas version: 1.3.5\n",
            "sklearn version: 1.0.2\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "\n",
        "print(f\"Python version: {python_version()}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"sklearn version: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we2tVCOslhVw"
      },
      "source": [
        "## Reused functions from Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV9mNjrz8Awv"
      },
      "outputs": [],
      "source": [
        "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
        "\n",
        "# FILTERING\n",
        "def create_column_filter(df):\n",
        "\n",
        "    # copy the input dataframe into df_copy\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    for colname, values in df.iteritems():\n",
        "        # only do something if it's not labeled 'CLASS' or 'ID'\n",
        "        if colname == \"CLASS\" or colname == \"ID\":\n",
        "          continue\n",
        "        else:\n",
        "            # Drop a column if it contains only missing values\n",
        "            df_copy = df_copy.dropna(how = 'all', axis=1)\n",
        "\n",
        "            # Drop a column if there's only one unique value...\n",
        "            arr = df[colname].unique()\n",
        "            # ... apart from the missing values\n",
        "            arr = arr[~ pd.isnull(arr)] # '~' = np not operator (!), isnull = True where nan\n",
        "            if arr.size == 1:\n",
        "                #if colname in df_copy.columns: # this line replaced with errors='ignore' below\n",
        "                df_copy = df_copy.drop([colname], axis=1, errors='ignore')\n",
        "            #alternative apart from the missing values\n",
        "            # elif arr.size == 2:\n",
        "            #   for i in range(arr.size):\n",
        "            #       if np.isnan(arr[i]):\n",
        "            #           df_copy = df_copy.drop([colname],axis=1, errors='ignore')\n",
        "\n",
        "\n",
        "    column_filter = df_copy.columns.values.tolist()\n",
        "\n",
        "    return df_copy, column_filter\n",
        "\n",
        "def apply_column_filter(df, column_filter):\n",
        "    return df.loc[:,column_filter] # : for all rows , column_filter cols\n",
        "\n",
        "\n",
        "#IMPUTATION\n",
        "def create_imputation(df):\n",
        "    dfcpy = df.copy()\n",
        "    imputation = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col == \"ID\" or col == \"CLASS\":\n",
        "            continue\n",
        "\n",
        "        coltype = dfcpy[col].dtypes # get type of column\n",
        "\n",
        "        if coltype == \"int64\" or coltype == \"float64\":\n",
        "            # Replace nan values with mean of column for numeric cols\n",
        "            mean = np.mean(dfcpy[col])\n",
        "\n",
        "            # Rare case - all values nan\n",
        "            if np.isnan(mean):\n",
        "                mean = 0\n",
        "\n",
        "            dfcpy[col] = dfcpy[col].fillna(mean)\n",
        "            imputation[col] = mean\n",
        "\n",
        "        if coltype == \"object\" or coltype == \"category\":\n",
        "            # Replace nan values with mode (most common value) for object/catg columns\n",
        "            mode = dfcpy[col].mode()\n",
        "\n",
        "            # Rare case - all values nan (the category one in Hint 4 doesnt make sense\n",
        "            # since category[0] = nan ...)\n",
        "            if mode.empty:\n",
        "                mode = \"\"\n",
        "            else:\n",
        "                mode = mode[0]\n",
        "\n",
        "            dfcpy[col] = dfcpy[col].fillna(mode)\n",
        "            imputation[col] = mode\n",
        "\n",
        "    return dfcpy, imputation\n",
        "\n",
        "def apply_imputation(df, imputation):\n",
        "    dfcpy = df.copy()\n",
        "    for col in df.columns:\n",
        "        if col == \"CLASS\" or col == \"ID\":\n",
        "            continue\n",
        "\n",
        "        # Check if this col was imputed\n",
        "        if col in imputation:\n",
        "            value = imputation[col] # get imputation (mean or mode)\n",
        "            dfcpy[col] = dfcpy[col].fillna(value)\n",
        "\n",
        "    return dfcpy\n",
        "\n",
        "\n",
        "\n",
        "#NORMALIZATION\n",
        "def create_normalization(df, normalizationtype='minmax'):\n",
        "    # copy the input dataframe into df_copy\n",
        "    df_copy = df.copy()\n",
        "    mapping = {}\n",
        "    for colname, values in df_copy.iteritems():\n",
        "\n",
        "        # only do something if it's not labeled 'CLASS' or 'ID'\n",
        "        if colname == \"CLASS\" or colname == \"ID\":\n",
        "          continue\n",
        "        else :\n",
        "            # Consider column of type \"float\" or \"int\"\n",
        "            # Normalization of the value\n",
        "            if df_copy[colname].dtype == 'float64' or 'int32' or 'int64' or 'float32':\n",
        "                # dictionary from each column name to a triple (\"minmax\",min_value,max_value) or (\"zscore\",mean,std)\n",
        "\n",
        "                if normalizationtype == \"zscore\":\n",
        "                    # z normalization\n",
        "                    mean = df_copy[colname].mean()\n",
        "                    std = df_copy[colname].std()\n",
        "                    #apply z normalization\n",
        "                    df_copy[colname] = df_copy[colname].apply(lambda x: (x-mean)/std)\n",
        "                    #filling the dictionary\n",
        "                    triple = [\"zscore\", mean, std]\n",
        "                    mapping[colname] = triple\n",
        "\n",
        "\n",
        "                else :\n",
        "                    #min-max normalization\n",
        "                    min = df_copy[colname].min()\n",
        "                    max = df_copy[colname].max()\n",
        "                    # apply min-max normalization\n",
        "                    df_copy[colname] = [(x-min)/(max-min) for x in df_copy[colname]]\n",
        "                    # store max,mean and mean for each column\n",
        "                    # for each column dictionary with three values (nested dictionary)\n",
        "                    triple = [\"minmax\", min, max]\n",
        "                    mapping[colname] = triple\n",
        "\n",
        "    #print(df_copy)\n",
        "\n",
        "    return df_copy, mapping\n",
        "\n",
        "def apply_normalization(df, mapping):\n",
        "    # copy the input dataframe into df_copy\n",
        "    df_copy = df.copy()\n",
        "    for colname, values in df_copy.iteritems():\n",
        "\n",
        "        # only do something if it's not labeled 'CLASS' or 'ID'\n",
        "        if colname == \"CLASS\" or colname == \"ID\":\n",
        "          continue;\n",
        "        else :\n",
        "            # Consider column of type \"float\" or \"int\"\n",
        "            # Normalization of the value\n",
        "            if df_copy[colname].dtype == 'float64' or 'int32' or 'int64' or 'float32':\n",
        "\n",
        "                if mapping.get(colname)[0] == 'zscore':\n",
        "                    # z normalization\n",
        "                    mean = mapping[colname][1]\n",
        "                    std = mapping[colname][2]\n",
        "                    #apply z normalization\n",
        "                    df_copy[colname] = df_copy[colname].apply(lambda x: (x-mean)/std)\n",
        "\n",
        "                else :\n",
        "                    #min-max normalization\n",
        "                    min = mapping[colname][1]\n",
        "                    max = mapping[colname][2]\n",
        "\n",
        "\n",
        "                    # apply min-max normalization\n",
        "                    # limit the output range to [0,1]\n",
        "\n",
        "                    df_copy[colname] = [1 if (x-min)/(max-min) > 1 else 0 if (x-min)/(max-min) < 0 else (x-min)/(max-min) for x in df_copy[colname]]\n",
        "\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "#ONE-HOT\n",
        "def create_one_hot(df):\n",
        "    # copy the input dataframe into df_copy and create an empy dictionary\n",
        "    df_copy = df.copy()\n",
        "    one_hot = {}\n",
        "    # only do something if the column is not labeled 'CLASS' or 'ID'\n",
        "    for colname, values in df_copy.iteritems():\n",
        "        if colname == \"CLASS\" or colname == \"ID\":\n",
        "            continue\n",
        "        else :\n",
        "            # consider column of type \"object\" or \"category\"\n",
        "            if df_copy[colname].dtype == 'object' or 'category':\n",
        "                # get the values and put them in the dictionary\n",
        "                one_hot[colname] = df_copy[colname].unique()\n",
        "                # create new columns with the one_hot encoding\n",
        "                for i, val in enumerate(values):\n",
        "                    # ADDED str() below due to new error (val == float64)\n",
        "                    df_copy[colname+\"_\"+str(val)] = df_copy[colname].apply(lambda x: 1 if x==val else 0)\n",
        "                # drop the old column\n",
        "                df_copy.drop(colname, axis=1, inplace=True)\n",
        "    return df_copy, one_hot\n",
        "\n",
        "def apply_one_hot(df, one_hot):\n",
        "    # copy the input dataframe into df_copy\n",
        "    df_copy = df.copy()\n",
        "    for colname, values in df_copy.iteritems():\n",
        "        for key, val in list(one_hot.items()):\n",
        "            # only do something if the column is already in the dictionary\n",
        "            if colname == key:\n",
        "                #create new columns with the one_hot encoding\n",
        "                for i, v in enumerate(val):\n",
        "                    # ADDED str() below due to new error (val == float64)\n",
        "                    df_copy[key+\"_\"+str(v)] = df_copy[key].apply(lambda x: 1 if x==v else 0)\n",
        "                #drop the old column\n",
        "                df_copy.drop(key, axis=1, inplace=True)\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "\n",
        "#ACCURACY\n",
        "def accuracy(df,correctlabels):\n",
        "\n",
        "    preds = df.idxmax(axis=1) # get index(col) of max value on axis=1(row-wise)\n",
        "    # print(correctlabels)\n",
        "    # print(type(correctlabels))\n",
        "\n",
        "    # print()\n",
        "    # print(preds)\n",
        "    # print(type(preds))\n",
        "\n",
        "    correct = np.sum(preds == correctlabels) # get nr of equal values in two arrays\n",
        "\n",
        "    acc = correct/len(correctlabels)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "#BRIER SCORE\n",
        "def brier_score(df, correctlabels):\n",
        "    #create matrix of zeros for true classes\n",
        "    t_class = np.zeros((len(df), len(np.unique(correctlabels))))\n",
        "    for colname, values in df.iteritems():\n",
        "        #get the index of the true class\n",
        "        ind = np.where(df.columns==colname)[0]\n",
        "        for i, v in enumerate(correctlabels):\n",
        "            # find the true class and change the values from 0 to 1\n",
        "            if v == colname:\n",
        "                t_class[i][ind] = 1\n",
        "    # brier_score\n",
        "    brier = np.mean(np.sum((df - t_class)**2, axis=1))\n",
        "    return brier\n",
        "\n",
        "# BINNING\n",
        "def create_bins(df, nobins = 10, bintype=\"equal-width\"):\n",
        "    # copy the input dataframe into df_copy and create an empty dictionary\n",
        "    df_copy = df.copy()\n",
        "    binning = {}\n",
        "    # only do something if the column is not labeled 'CLASS' or 'ID'\n",
        "    for colname, values in df_copy.iteritems():\n",
        "        if colname == \"CLASS\" or colname == \"ID\":\n",
        "            continue;\n",
        "        else :\n",
        "            # only consider column of type \"float\" or \"int\"\n",
        "            if df_copy[colname].dtype == 'float64' or 'int32' or 'int64' or 'float32':\n",
        "                # create the categories for each value and the tresholds for the bins\n",
        "                if bintype == \"equal-width\":\n",
        "                    cat = pd.cut(df_copy[colname], bins=nobins, labels=False, retbins = True, duplicates = \"drop\")\n",
        "                elif bintype == \"equal-size\":\n",
        "                    cat = pd.qcut(df_copy[colname], q=nobins, labels=False, retbins = True, duplicates = \"drop\")\n",
        "                # replace with the categories and change column type to \"category\"\n",
        "                df_copy[colname] = cat[0].astype('category')\n",
        "                # change the first and the last element of each binning to -np.inf and np.inf\n",
        "                cat[1][0] = -np.inf\n",
        "                cat[1][-1] = np.inf\n",
        "                # add to dictionary\n",
        "                binning[colname] = cat[1]\n",
        "    return df_copy, binning\n",
        "\n",
        "\n",
        "\n",
        "def apply_bins(df, binning):\n",
        "    # copy the input dataframe into df_copy\n",
        "    df_copy = df.copy()\n",
        "    for colname, values in df_copy.iteritems():\n",
        "       # only do something if the column is already in the dictionary\n",
        "       for key, value in list(binning.items()):\n",
        "            if colname == key:\n",
        "                # create the category based on the bins provided in binning\n",
        "                cat = pd.cut(df_copy[colname], bins=binning[colname], labels=False, retbins = False, duplicates = \"drop\")\n",
        "                # replace with the categories and change type to \"category\"\n",
        "                df_copy[colname] = cat.astype('category')\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "#AUC\n",
        "def auc(df,correctlabels):\n",
        "    all_dicts = []\n",
        "\n",
        "    # Create triple dictionaries for each class\n",
        "    for col in df.columns:\n",
        "\n",
        "        # Initialize dictionary with s_i --> [0,0]\n",
        "        triples = {}\n",
        "        for prob in df[col]:\n",
        "            triples[prob] = [0,0]\n",
        "\n",
        "        # Generate dictionary\n",
        "        i = 0\n",
        "        for prob in df[col]:\n",
        "            # number of istances in that class\n",
        "            if correctlabels[i] == col:\n",
        "                triples[prob][0] += 1\n",
        "            # number of istances not in that class\n",
        "            else:\n",
        "                triples[prob][1] += 1\n",
        "            i+=1\n",
        "\n",
        "        # print(f\"Triples for {col}:\")\n",
        "        # print(triples)\n",
        "        # print()\n",
        "        all_dicts.append(triples)\n",
        "\n",
        "    AUCs = [] # list for all AUCs\n",
        "\n",
        "    # Calculate AUC for each triple dictionary\n",
        "    for dicts in all_dicts:\n",
        "\n",
        "        # Calculate total tps and fps\n",
        "        tot_tp = 0\n",
        "        tot_fp = 0\n",
        "        # print(\"Probability ordering:\")\n",
        "        for prob in sorted(dicts,reverse=True):\n",
        "            # print(prob)\n",
        "            pair = dicts[prob]\n",
        "            tot_tp += pair[0]\n",
        "            tot_fp += pair[1]\n",
        "        # print()\n",
        "\n",
        "        # Begin AUC algorithm\n",
        "        single_AUC = 0\n",
        "        cov_tp = 0\n",
        "\n",
        "        # Loop through the sorted pairs\n",
        "        for prob in sorted(dicts,reverse=True):\n",
        "            pair = dicts[prob]\n",
        "            tp = pair[0]\n",
        "            fp = pair[1]\n",
        "\n",
        "            if fp == 0:\n",
        "                cov_tp += tp\n",
        "            elif tp == 0:\n",
        "                single_AUC += (cov_tp/tot_tp) * (fp/tot_fp)\n",
        "            else:\n",
        "                single_AUC += (cov_tp/tot_tp) * (fp/tot_fp) + ((tp/tot_tp) * (fp/tot_fp))/2\n",
        "                cov_tp += tp\n",
        "\n",
        "        AUCs.append(single_AUC)\n",
        "\n",
        "    # print(\"All AUCs:\")\n",
        "    # print(AUCs)\n",
        "    # print()\n",
        "\n",
        "    # Count how many times each col appears in correct labels\n",
        "    col_correct = []\n",
        "    for col in df.columns:\n",
        "        correct_count = 0\n",
        "        for label in correctlabels:\n",
        "            if col == label:\n",
        "                correct_count += 1\n",
        "        # print(f\"{col} is correct {correct_count} times\")\n",
        "        col_correct.append(correct_count)\n",
        "\n",
        "    # Compute final weighted AUC\n",
        "    AUC = 0\n",
        "    for i in range(len(AUCs)):\n",
        "        weight = col_correct[i]/len(correctlabels)\n",
        "        AUC += weight * AUCs[i]\n",
        "\n",
        "    # print()\n",
        "    return AUC\n",
        "\n",
        "\n",
        "#EUCLIDEAN DISTANCE\n",
        "def eucledian(p1,p2):\n",
        "    dist = np.sqrt(np.sum((p1-p2)**2))\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUe0wPQxlhVy"
      },
      "source": [
        "## 1. Define the class RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz5WtEdvlhVy"
      },
      "outputs": [],
      "source": [
        "# Define the class RandomForest with three functions __init__, fit and predict (after the comments):\n",
        "class RandomForest:\n",
        "# Input to __init__:\n",
        "# self - the object itself\n",
        "#\n",
        "# Output from __init__:\n",
        "# <nothing>\n",
        "#\n",
        "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
        "# column_filter, imputation, one_hot, labels, model\n",
        "    def __init__(self):\n",
        "        self.column_filter = None\n",
        "        self.imputation = None\n",
        "        self.one_hot = None\n",
        "        self.labels = None\n",
        "        self.model = None\n",
        "\n",
        "# Input to fit:\n",
        "# self      - the object itself\n",
        "# df        - a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
        "# no_trees  - no. of trees in the random forest (default = 100)\n",
        "#\n",
        "# Output from fit:\n",
        "# <nothing>\n",
        "#\n",
        "# The result of applying this function should be:\n",
        "#\n",
        "# self.column_filter - a column filter (see Assignment 1) from df\n",
        "# self.imputation    - an imputation mapping (see Assignment 1) from df\n",
        "# self.one_hot       - a one-hot mapping (see Assignment 1) from df\n",
        "# self.labels        - a (sorted) list of the categories of the \"CLASS\" column of df\n",
        "# self.model         - a random forest, consisting of no_trees trees, where each tree is generated from a bootstrap sample\n",
        "#                      and the number of evaluated features is log2|F| where |F| is the total number of features\n",
        "#                      (for details, see lecture slides)\n",
        "#\n",
        "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
        "#\n",
        "# Hint 1: First create the column filter, imputation and one-hot mappings\n",
        "#\n",
        "# Hint 2: Then get the class labels and the numerical values (as an ndarray) from the dataframe after dropping the class labels\n",
        "#\n",
        "# Hint 3: Generate no_trees classification trees, where each tree is generated using DecisionTreeClassifier\n",
        "#         from a bootstrap sample (see lecture slides), e.g., generated by np.random.choice (with replacement)\n",
        "#         from the row numbers of the ndarray, and where a random sample of the features are evaluated in\n",
        "#         each node of each tree, of size log2(|F|), where |F| is the total number of features;\n",
        "#         see the parameter max_features of DecisionTreeClassifier\n",
        "    def generate_forest(self, no_trees, df):\n",
        "        forest = []\n",
        "        for i in range(no_trees):\n",
        "\n",
        "            # Get bootstrap sample inidices\n",
        "            rows = [i for i in range(len(df))]\n",
        "            no_instances = len(rows)\n",
        "            bootstrap_rows = np.random.choice(rows,size=no_instances,replace=True)\n",
        "\n",
        "            # Separate Training instances, ID and CLASSES\n",
        "            # Construct bootstrap samples\n",
        "            bootstrap_classes = df['CLASS'].values\n",
        "            bootstrap_classes = bootstrap_classes[bootstrap_rows]\n",
        "            bootstrap_instances = df.drop('CLASS',axis=1).values\n",
        "            bootstrap_instances = bootstrap_instances[bootstrap_rows,:]\n",
        "\n",
        "            # Create a tree\n",
        "            # Number of features\n",
        "            no_features = int(np.log2(bootstrap_instances.shape[1]))\n",
        "            tree = DecisionTreeClassifier(max_features=no_features)\n",
        "            tree.fit(bootstrap_instances,bootstrap_classes)\n",
        "\n",
        "            # Print tree\n",
        "            #tree_desc = tree.export_text(tree, feature_names=list(df.columns))\n",
        "            forest.append(tree)\n",
        "\n",
        "        return forest\n",
        "\n",
        "    def fit(self, df, no_trees=100):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        # df_flitered : new df where columns containing only missing values have been dropped\n",
        "        # col_filter : list of the names of the remaining columns\n",
        "        df_filtered, col_filter = create_column_filter(dfcpy)\n",
        "        self.column_filter = col_filter\n",
        "\n",
        "        # df_imputed : new df missing numeric value in a column has been replaced by the mean of that column\n",
        "        # missing categoric value in a column has been replaced by the mode of that column imputation\n",
        "        # impt: mapping (dictionary) from column name to value that has replaced missing values\n",
        "        df_imputed, impt = create_imputation(df_filtered)\n",
        "        self.imputation = impt\n",
        "\n",
        "        #df_onehot: new df where each categoric feature has been replaced by a set of binary features\n",
        "        # one_hot: dictionary from column name to a set of categories (possible values for the feature)\n",
        "        # is the hot encoding right because also numerical values are here encoded?\n",
        "        df_onehot, one_h = create_one_hot(df_imputed)\n",
        "        self.one_hot = one_h\n",
        "\n",
        "        # list of class labels\n",
        "        self.labels = sorted(dfcpy[\"CLASS\"].unique())\n",
        "\n",
        "        random_forest = self.generate_forest(no_trees,df_onehot)\n",
        "        self.model = random_forest\n",
        "\n",
        "# Input to predict:\n",
        "# self - the object itself\n",
        "# df   - a dataframe\n",
        "#\n",
        "# Output from predict:\n",
        "# predictions - a dataframe with class labels as column names and the rows corresponding to\n",
        "#               predictions with estimated class probabilities for each row in df, where the class probabilities\n",
        "#               are the averaged probabilities output by each decision tree in the forest\n",
        "#\n",
        "# Hint 1: Drop any \"CLASS\" and \"ID\" columns of the dataframe first and then apply column filter, imputation and one_hot\n",
        "#\n",
        "# Hint 2: Iterate over the trees in the forest to get the prediction of each tree by the method predict_proba(X) where\n",
        "#         X are the (numerical) values of the transformed dataframe; you may get the average predictions of all trees,\n",
        "#         by first creating a zero-matrix with one row for each test instance and one column for each class label,\n",
        "#         to which you add the prediction of each tree on each iteration, and then finally divide the prediction matrix\n",
        "#         by the number of trees.\n",
        "#\n",
        "# Hint 3: You may assume that each bootstrap sample that was used to generate each tree has included all possible\n",
        "#         class labels and hence the prediction of each tree will contain probabilities for all class labels\n",
        "#         (in the same order). Note that this assumption may be violated, and this limitation will be addressed\n",
        "#         in the next part of the assignment.\n",
        "    def predict(self,df):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        df_filtered = apply_column_filter(dfcpy,self.column_filter)\n",
        "        df_imputed = apply_imputation(df_filtered,self.imputation)\n",
        "        df_onehot = apply_one_hot(df_imputed,self.one_hot)\n",
        "        df_onehot = df_onehot.drop([\"CLASS\"],axis=1)\n",
        "\n",
        "        data = df_onehot.values\n",
        "\n",
        "        # Prediction matrix with test instance no of rows and class labels no of cols\n",
        "        prediction_matrix = np.zeros((data.shape[0],len(self.labels)),dtype='float64')\n",
        "\n",
        "        no_trees = 0\n",
        "        for tree in self.model:\n",
        "            # Prediction of tree has class labels x instances no of predictions\n",
        "            pred = tree.predict_proba(data)\n",
        "\n",
        "            prediction_matrix += pred\n",
        "            no_trees += 1\n",
        "\n",
        "        # Divide prediction matrix by number of trees\n",
        "        prediction_matrix /= no_trees\n",
        "        prediction_matrix = pd.DataFrame(prediction_matrix,columns=self.labels)\n",
        "\n",
        "        return prediction_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc66LIq7lhVz",
        "outputId": "86e717d2-a8c9-40f3-c8c1-80fedb47e236",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 2.60 s.\n",
            "Testing time: 0.06 s.\n",
            "Accuracy: 0.9061\n",
            "AUC: 0.9894\n",
            "Brier score: 0.1780\n"
          ]
        }
      ],
      "source": [
        "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
        "\n",
        "train_df = pd.read_csv(filepath+\"tic-tac-toe_train.csv\")\n",
        "\n",
        "test_df = pd.read_csv(filepath+\"tic-tac-toe_test.csv\")\n",
        "\n",
        "rf = RandomForest()\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rf.fit(train_df)\n",
        "print(\"Training time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "test_labels = test_df[\"CLASS\"]\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "predictions = rf.predict(test_df)\n",
        "\n",
        "print(\"Testing time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy(predictions,test_labels)))\n",
        "print(\"AUC: {:.4f}\".format(auc(predictions,test_labels))) # Comment this out if not implemented in assignment 1\n",
        "print(\"Brier score: {:.4f}\".format(brier_score(predictions,test_labels))) # Comment this out if not implemented in assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzhI8QVhlhVz",
        "outputId": "5daa97a5-6a34-48e0-9888-4b928320a8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 1.0000\n",
            "AUC on training set: 1.0000\n",
            "Brier score on training set: 0.0209\n"
          ]
        }
      ],
      "source": [
        "train_labels = train_df[\"CLASS\"]\n",
        "predictions = rf.predict(train_df)\n",
        "print(\"Accuracy on training set: {0:.4f}\".format(accuracy(predictions,train_labels)))\n",
        "print(\"AUC on training set: {0:.4f}\".format(auc(predictions,train_labels))) # Comment this out if not implemented in assignment 1\n",
        "print(\"Brier score on training set: {0:.4f}\".format(brier_score(predictions,train_labels))) # Comment this out if not implemented in assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUJtA6aclhV0"
      },
      "source": [
        "## 2a. Handling trees with non-aligned predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgP03PpqlhV0"
      },
      "outputs": [],
      "source": [
        "# Define a revised version of the class RandomForest with the same input and output as described in part 1 above,\n",
        "# where the predict function is able to handle the case where the individual trees are trained on bootstrap samples\n",
        "# that do not include all class labels in the original training set. This leads to that the class probabilities output\n",
        "# by the individual trees in the forest do not refer to the same set of class labels.\n",
        "#\n",
        "# Hint 1: The categories obtained with <pandas series>.cat.categories are sorted in the same way as the class labels\n",
        "#         of a DecisionTreeClassifier; the latter are obtained by <DecisionTreeClassifier>.classes_\n",
        "#         The problem is that classes_ may not include all possible labels, and hence the individual predictions\n",
        "#         obtained by <DecisionTreeClassifier>.predict_proba may be of different length or even if they are of the same\n",
        "#         length do not necessarily refer to the same class labels. You may assume that each class label that is not included\n",
        "#         in a bootstrap sample should be assigned zero probability by the tree generated from the bootstrap sample.\n",
        "#\n",
        "# Hint 2: Create a mapping from the complete (and sorted) set of class labels l0, ..., lk-1 to a set of indexes 0, ..., k-1,\n",
        "#         where k is the number of classes\n",
        "#\n",
        "# Hint 3: For each tree t in the forest, create a (zero) matrix with one row per test instance and one column per class label,\n",
        "#         to which one column is added at a time from the output of t.predict_proba\n",
        "#\n",
        "# Hint 4: For each column output by t.predict_proba, its index i may be used to obtain its label by t.classes_[i];\n",
        "#         you may then obtain the index of this label in the ordered list of all possible labels from the above mapping (hint 2);\n",
        "#         this index points to which column in the prediction matrix the output column should be added to\n",
        "\n",
        "class RandomForest:\n",
        "# Input to __init__:\n",
        "# self - the object itself\n",
        "#\n",
        "# Output from __init__:\n",
        "# <nothing>\n",
        "#\n",
        "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
        "# column_filter, imputation, one_hot, labels, model\n",
        "    def __init__(self):\n",
        "        self.column_filter = None\n",
        "        self.imputation = None\n",
        "        self.one_hot = None\n",
        "        self.labels = None\n",
        "        self.model = None\n",
        "\n",
        "    def generate_forest(self, no_trees, df):\n",
        "        forest = []\n",
        "        for i in range(no_trees):\n",
        "\n",
        "            # Get bootstrap sample inidices\n",
        "            rows = [i for i in range(len(df))]\n",
        "            no_instances = len(rows)\n",
        "            bootstrap_rows = np.random.choice(rows,size=no_instances,replace=True)\n",
        "\n",
        "            # Separate Training instances, ID and CLASSES\n",
        "            # Construct bootstrap samples\n",
        "            bootstrap_classes = df['CLASS'].values\n",
        "            bootstrap_classes = bootstrap_classes[bootstrap_rows]\n",
        "            bootstrap_instances = df.drop('CLASS',axis=1).values\n",
        "            bootstrap_instances = bootstrap_instances[bootstrap_rows,:]\n",
        "\n",
        "            # Create a tree\n",
        "            # Number of features\n",
        "            no_features = int(np.log2(bootstrap_instances.shape[1]))\n",
        "            tree = DecisionTreeClassifier(max_features=no_features)\n",
        "            tree.fit(bootstrap_instances,bootstrap_classes)\n",
        "            # print(tree.classes_)\n",
        "\n",
        "            # Print tree\n",
        "            #tree_desc = tree.export_text(tree, feature_names=list(df.columns))\n",
        "            forest.append(tree)\n",
        "\n",
        "        return forest\n",
        "\n",
        "    def fit(self, df, no_trees=100):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        # df_flitered : new df where columns containing only missing values have been dropped\n",
        "        # col_filter : list of the names of the remaining columns\n",
        "        df_filtered, col_filter = create_column_filter(dfcpy)\n",
        "        self.column_filter = col_filter\n",
        "\n",
        "        # df_imputed : new df missing numeric value in a column has been replaced by the mean of that column\n",
        "        # missing categoric value in a column has been replaced by the mode of that column imputation\n",
        "        # impt: mapping (dictionary) from column name to value that has replaced missing values\n",
        "        df_imputed, impt = create_imputation(df_filtered)\n",
        "        self.imputation = impt\n",
        "\n",
        "        #df_onehot: new df where each categoric feature has been replaced by a set of binary features\n",
        "        # one_hot: dictionary from column name to a set of categories (possible values for the feature)\n",
        "        # is the hot encoding right because also numerical values are here encoded?\n",
        "        df_onehot, one_h = create_one_hot(df_imputed)\n",
        "        self.one_hot = one_h\n",
        "\n",
        "       # list of class labels\n",
        "        self.labels = sorted(dfcpy[\"CLASS\"].unique())\n",
        "\n",
        "        random_forest = self.generate_forest(no_trees,df_onehot)\n",
        "        self.model = random_forest\n",
        "\n",
        "    def predict(self,df):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        df_filtered = apply_column_filter(dfcpy,self.column_filter)\n",
        "        df_imputed = apply_imputation(df_filtered,self.imputation)\n",
        "        df_onehot = apply_one_hot(df_imputed,self.one_hot)\n",
        "        df_onehot = df_onehot.drop([\"CLASS\"],axis=1)\n",
        "\n",
        "        data = df_onehot.values\n",
        "\n",
        "        # create a mapping between n sorted class labels and their indices from 0 to n-1\n",
        "        mapping = {}\n",
        "        for index, label in enumerate(self.labels):\n",
        "            mapping[label] = index\n",
        "\n",
        "        # zero matrix test instance nr of rows and as column the class labels\n",
        "        prediction_matrix = np.zeros((data.shape[0],len(self.labels)), dtype='float64')\n",
        "\n",
        "        # go through all trees in the random forest\n",
        "        no_trees = 0\n",
        "        for t in self.model:\n",
        "            # array with the probailities of each class appearing in the tree t\n",
        "            proba = t.predict_proba(data)\n",
        "\n",
        "            i = 0\n",
        "            for index in range(len(self.labels)):\n",
        "                #if the values in the labels and labels tree are equal\n",
        "                # add to to the column with corresponding indice (from the mapping for this given class) the probability in this tree of this given glass\n",
        "                # t.classes_ array of the classes present in t, sorted in the same order as t.predict_proba_\n",
        "                if self.labels[index]==t.classes_[i]:\n",
        "                    prediction_matrix[:,mapping[self.labels[index]]] += proba[:,i]\n",
        "                    # go to the next value in the proba array and class tree array\n",
        "                    i = i+1\n",
        "                else:\n",
        "                    prediction_matrix[:,mapping[self.labels[index]]] += 0\n",
        "\n",
        "            # go to the next elements in class\n",
        "            no_trees += 1\n",
        "\n",
        "        # divide summed probabilities with no of trees\n",
        "        prediction_matrix /= no_trees\n",
        "\n",
        "        prediction_matrix = pd.DataFrame(prediction_matrix,columns=self.labels)\n",
        "\n",
        "        return prediction_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP2vaWWXlhV0",
        "outputId": "a2977a55-f709-4264-d7b3-8d5bd27b0ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 5.08 s.\n",
            "Testing time: 0.24 s.\n",
            "Accuracy: 0.9354\n",
            "AUC: 0.9746\n",
            "Brier score: 0.1203\n"
          ]
        }
      ],
      "source": [
        "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
        "\n",
        "train_df = pd.read_csv(filepath+\"anneal_train.csv\")\n",
        "\n",
        "test_df = pd.read_csv(filepath+\"anneal_test.csv\")\n",
        "\n",
        "rf = RandomForest()\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rf.fit(train_df)\n",
        "\n",
        "\n",
        "print(\"Training time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "test_labels = test_df[\"CLASS\"]\n",
        "t0 = time.perf_counter()\n",
        "predictions = rf.predict(test_df)\n",
        "print(\"Testing time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy(predictions,test_labels)))\n",
        "print(\"AUC: {:.4f}\".format(auc(predictions,test_labels))) # Comment this out if not implemented in assignment 1\n",
        "print(\"Brier score: {:.4f}\".format(brier_score(predictions,test_labels))) # Comment this out if not implemented in assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SpzTuTblhV1"
      },
      "source": [
        "## 2b. Estimate predictive performance using out-of-bag predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocHGFdh1lhV1"
      },
      "outputs": [],
      "source": [
        "# Define an extended version of the class RandomForest with the same input and output as described in part 2a above,\n",
        "# where the results of the fit function also should include:\n",
        "# self.oob_acc - the accuracy estimated on the out-of-bag predictions, i.e., the fraction of training instances for\n",
        "#                which the given (correct) label is the same as the predicted label when using only trees for which\n",
        "#                the instance is out-of-bag\n",
        "#\n",
        "# Hint 1: You may first create a zero matrix with one row for each training instance and one column for each class label\n",
        "#         and one zero vector to allow for storing aggregated out-of-bag predictions and the number of out-of-bag predictions\n",
        "#         for each training instance, respectively. By \"aggregated out-of-bag predictions\" is here meant the sum of all\n",
        "#         predicted probabilities (one sum per class and instance). These sums should be divided by the number of predictions\n",
        "#         (stored in the vector) in order to obtain a single class probability distribution per training instance.\n",
        "#         This distribution is considered to be the out-of-bag prediction for each instance, and e.g., the class that\n",
        "#         receives the highest probability for each instance can be compared to the correct label of the instance,\n",
        "#         when calculating the accuracy using the out-of-bag predictions.\n",
        "#\n",
        "# Hint 2: After generating a tree in the forest, iterate over the indexes that were not included in the bootstrap sample\n",
        "#         and add a prediction of the tree to the out-of-bag prediction matrix and update the count vector\n",
        "#\n",
        "# Hint 3: Note that the input to predict_proba has to be a matrix; from a single vector (row) x, a matrix with one row\n",
        "#         can be obtained by x[None,:]\n",
        "#\n",
        "# Hint 4: Finally, divide each row in the out-of-bag prediction matrix with the corresponding element of the count vector\n",
        "#\n",
        "#         For example, assuming that we have two class labels, then we may end up with the following matrix:\n",
        "#\n",
        "#         2 4\n",
        "#         4 4\n",
        "#         5 0\n",
        "#         ...\n",
        "#\n",
        "#         and the vector (no. of predictions) (6, 8, 5, ...)\n",
        "#\n",
        "#         The resulting class probability distributions are:\n",
        "#\n",
        "#         0.333... 0.666...\n",
        "#         0.5 0.5\n",
        "#         1.0 0\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "# Input to __init__:\n",
        "# self - the object itself\n",
        "#\n",
        "# Output from __init__:\n",
        "# <nothing>\n",
        "#\n",
        "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
        "# column_filter, imputation, one_hot, labels, model\n",
        "    def __init__(self):\n",
        "        self.column_filter = None\n",
        "        self.imputation = None\n",
        "        self.one_hot = None\n",
        "        self.labels = None\n",
        "        self.model = None\n",
        "        self.obb_acc = None\n",
        "\n",
        "    def generate_forest(self, no_trees, df):\n",
        "\n",
        "        #create all lists and matrices needed\n",
        "        forest = []\n",
        "        oob_matrix = np.zeros((self.training_data.shape[0], len(self.labels)), dtype='float64')\n",
        "        val_oob = np.zeros(self.training_data.shape[0])\n",
        "\n",
        "        for t in range(no_trees):\n",
        "            #get bootstrap sample indices\n",
        "            rows = [t for t in range(len(df))]\n",
        "            no_instances = len(rows)\n",
        "            bootstrap_rows = np.random.choice(rows,size=no_instances,replace=True)\n",
        "\n",
        "            #get out of bag sample indices\n",
        "            oob_rows = []\n",
        "            for i in range(len(df)):\n",
        "                if i not in bootstrap_rows:\n",
        "                    oob_rows.append(i)\n",
        "\n",
        "            # Separate Training instances and CLASSES\n",
        "            # Construct bootstrap samples\n",
        "            bootstrap_classes = df['CLASS'].values\n",
        "            bootstrap_classes = bootstrap_classes[bootstrap_rows]\n",
        "            bootstrap_instances = df.drop('CLASS',axis=1).values\n",
        "            bootstrap_instances = bootstrap_instances[bootstrap_rows,:]\n",
        "\n",
        "            # Separate Training instances and CLASSES\n",
        "            # Construct out of bag samples\n",
        "            oob_instances = df.drop('CLASS',axis=1).values\n",
        "            oob_instances = oob_instances[oob_rows,:]\n",
        "\n",
        "            # Create a tree\n",
        "            # Number of features\n",
        "            no_features = int(np.log2(bootstrap_instances.shape[1]))\n",
        "            tree = DecisionTreeClassifier(max_features=no_features)\n",
        "            tree.fit(bootstrap_instances, bootstrap_classes)\n",
        "\n",
        "            #for each oob instance predict the class\n",
        "            for row in oob_rows:\n",
        "                i = 0\n",
        "                oob_pred = tree.predict_proba(oob_instances[i][None,:])\n",
        "                #add a oob sample to the count\n",
        "                val_oob[i] +=1\n",
        "\n",
        "                for index in range(len(self.labels)):\n",
        "\n",
        "                    #if the values in the labels and labels tree are the same, assign probability showed in oob_pred\n",
        "                    if self.labels[index]==tree.classes_[i]:\n",
        "                        oob_matrix[row, self.mapping[self.labels[index]]] += oob_pred[0, i]\n",
        "                    #if the values in the labels and labels tree are different assign probability 0 to the labels\n",
        "                    else:\n",
        "                        oob_matrix[:, self.mapping[self.labels[index]]] += 0\n",
        "                i = i+1\n",
        "\n",
        "            forest.append(tree)\n",
        "\n",
        "        #divide each element by the number of trees that it was built on\n",
        "        for i in range(oob_matrix.shape[0]):\n",
        "            for j in range(oob_matrix.shape[1]):\n",
        "                if oob_matrix[i,j]!= 0 :\n",
        "                    oob_matrix[i,j] /= val_oob[i]\n",
        "\n",
        "        #create a dataframe and select the correctlabels\n",
        "        oob_matrix = pd.DataFrame(oob_matrix, columns=self.class_val.unique())\n",
        "        correctlabels = self.class_val\n",
        "\n",
        "        #calculate accuracy\n",
        "        self.oob_acc = accuracy(oob_matrix, correctlabels)\n",
        "\n",
        "        return forest, self.oob_acc\n",
        "\n",
        "    def fit(self, df, no_trees=100):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        # df_flitered : new df where columns containing only missing values have been dropped\n",
        "        # col_filter : list of the names of the remaining columns\n",
        "        df_filtered, col_filter = create_column_filter(dfcpy)\n",
        "        self.column_filter = col_filter\n",
        "\n",
        "        # df_imputed : new df missing numeric value in a column has been replaced by the mean of that column\n",
        "        # missing categoric value in a column has been replaced by the mode of that column imputation\n",
        "        # impt: mapping (dictionary) from column name to value that has replaced missing values\n",
        "        df_imputed, impt = create_imputation(df_filtered)\n",
        "        self.imputation = impt\n",
        "\n",
        "        #df_onehot: new df where each categoric feature has been replaced by a set of binary features\n",
        "        # one_hot: dictionary from column name to a set of categories (possible values for the feature)\n",
        "        # is the hot encoding right because also numerical values are here encoded?\n",
        "        df_onehot, one_h = create_one_hot(df_imputed)\n",
        "        self.one_hot = one_h\n",
        "\n",
        "        # list of class labels\n",
        "        self.labels = sorted(dfcpy[\"CLASS\"].unique())\n",
        "\n",
        "        #create y dataframe with classes\n",
        "        self.class_val = df_onehot['CLASS']\n",
        "        self.training_data = df_onehot\n",
        "\n",
        "        #create mapping of\n",
        "        self.mapping = {}\n",
        "        for index, label in enumerate(self.labels):\n",
        "            self.mapping[label] = index\n",
        "\n",
        "        #create tghe forest with the function \"generate_forest\"\n",
        "        random_forest, oob = self.generate_forest(no_trees,df_onehot)\n",
        "        self.model = random_forest\n",
        "        self.oob = oob\n",
        "\n",
        "    def predict(self,df):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        #pre-proccessing of the dataframe\n",
        "        df_filtered = apply_column_filter(dfcpy,self.column_filter)\n",
        "        df_imputed = apply_imputation(df_filtered,self.imputation)\n",
        "        df_onehot = apply_one_hot(df_imputed,self.one_hot)\n",
        "        df_onehot = df_onehot.drop([\"CLASS\"],axis=1)\n",
        "\n",
        "        data = df_onehot.values\n",
        "\n",
        "        # zero matrix as rows number of test instances and as column the class labels\n",
        "        prediction_matrix = np.zeros((data.shape[0],len(self.labels)), dtype='float64')\n",
        "\n",
        "        no_trees = 0\n",
        "        for t in self.model:\n",
        "            #predict on the test data\n",
        "            proba = t.predict_proba(data)\n",
        "            i = 0\n",
        "            for index in range(len(self.labels)):\n",
        "                #if the values in the labels and labels tree are the same, assign probability showed in proba\n",
        "                if self.labels[index]==t.classes_[i]:\n",
        "                    prediction_matrix[:,self.mapping[self.labels[index]]] += proba[:,i]\n",
        "                    i = i+1\n",
        "                #if the values in the labels and labels tree are different assign probability 0 to the labels\n",
        "                else:\n",
        "                    prediction_matrix[:,self.mapping[self.labels[index]]] += 0\n",
        "            # go to the next elements in class\n",
        "            no_trees += 1\n",
        "\n",
        "        prediction_matrix /= no_trees\n",
        "        prediction_matrix = pd.DataFrame(prediction_matrix,columns=self.labels)\n",
        "\n",
        "        return prediction_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHctVWdIlhV2",
        "outputId": "6ab6effd-53c1-49e2-ae0d-bdf9e419038e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 6.81 s.\n",
            "OOB accuracy: 0.7550\n",
            "Testing time: 0.24 s.\n",
            "Accuracy: 0.9354\n",
            "AUC: 0.9757\n",
            "Brier score: 0.1192\n"
          ]
        }
      ],
      "source": [
        "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
        "\n",
        "train_df = pd.read_csv(filepath+\"anneal_train.csv\")\n",
        "\n",
        "test_df = pd.read_csv(filepath+\"anneal_test.csv\")\n",
        "\n",
        "rf = RandomForest()\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rf.fit(train_df)\n",
        "print(\"Training time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"OOB accuracy: {:.4f}\".format(rf.oob_acc))\n",
        "test_labels = test_df[\"CLASS\"]\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "predictions = rf.predict(test_df)\n",
        "print(\"Testing time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy(predictions,test_labels)))\n",
        "print(\"AUC: {:.4f}\".format(auc(predictions,test_labels))) # Comment this out if not implemented in assignment 1\n",
        "print(\"Brier score: {:.4f}\".format(brier_score(predictions,test_labels))) # Comment this out if not implemented in assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsHPNxJklhV2",
        "outputId": "0a86863c-8eaf-4bcd-eb07-538bfd1e0b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 1.00\n",
            "AUC on training set: 1.00\n",
            "Brier score on training set: 0.02\n"
          ]
        }
      ],
      "source": [
        "train_labels = train_df[\"CLASS\"]\n",
        "rf = RandomForest()\n",
        "rf.fit(train_df)\n",
        "predictions = rf.predict(train_df)\n",
        "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
        "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
        "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcKX2ggLRDZ9"
      },
      "source": [
        "### 2b for tic-tac-toe\n",
        "\n",
        "If implemented on a dataset based on exercise 1, the algorithm provides a better OOB accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4vUT00lM6wd"
      },
      "outputs": [],
      "source": [
        "class RandomForest:\n",
        "    def __init__(self):\n",
        "        self.column_filter = None\n",
        "        self.imputation = None\n",
        "        self.one_hot = None\n",
        "        self.labels = None\n",
        "        self.model = None\n",
        "        self.obb_acc = None\n",
        "\n",
        "\n",
        "    def generate_forest(self, no_trees, df):\n",
        "        forest = []\n",
        "        oob_matrix = np.zeros((self.training_data.shape[0], len(self.labels)), dtype='float64')\n",
        "        val_oob = np.zeros(self.training_data.shape[0])\n",
        "\n",
        "        for t in range(no_trees):\n",
        "\n",
        "            # Get bootstrap sample inidices\n",
        "            rows = [t for t in range(len(df))]\n",
        "            no_instances = len(rows)\n",
        "            bootstrap_rows = np.random.choice(rows,size=no_instances,replace=True)\n",
        "\n",
        "            oob_rows = []\n",
        "            for i in range(len(df)):\n",
        "                if i not in bootstrap_rows:\n",
        "                    oob_rows.append(i)\n",
        "\n",
        "            # Separate Training instances and CLASSES\n",
        "            # Construct bootstrap samples\n",
        "            bootstrap_classes = df['CLASS'].values\n",
        "            bootstrap_classes = bootstrap_classes[bootstrap_rows]\n",
        "            bootstrap_instances = df.drop('CLASS',axis=1).values\n",
        "            bootstrap_instances = bootstrap_instances[bootstrap_rows,:]\n",
        "\n",
        "            oob_instances = df.drop('CLASS',axis=1).values\n",
        "            oob_instances = oob_instances[oob_rows,:]\n",
        "\n",
        "            # Create a tree\n",
        "            # Number of features\n",
        "            no_features = int(np.log2(bootstrap_instances.shape[1]))\n",
        "            tree = DecisionTreeClassifier(max_features=no_features)\n",
        "            tree.fit(bootstrap_instances, bootstrap_classes)\n",
        "\n",
        "            #the predictions can be added directly to the rows of the matrix since\n",
        "            #all the classes are in every tree\n",
        "            oob_pred = tree.predict_proba(oob_instances)\n",
        "            ind = 0\n",
        "            for i in oob_rows:\n",
        "                oob_matrix[i,:] += oob_pred[ind]\n",
        "                val_oob[i] +=1\n",
        "                ind +=1\n",
        "\n",
        "            forest.append(tree)\n",
        "\n",
        "        for row in range(oob_matrix.shape[0]):\n",
        "            oob_matrix[row,:] /=  val_oob[row]\n",
        "\n",
        "\n",
        "        oob_matrix = pd.DataFrame(oob_matrix, columns=self.class_val.unique())\n",
        "        correctlabels = self.class_val\n",
        "\n",
        "        self.oob_acc = accuracy(oob_matrix, correctlabels)\n",
        "\n",
        "        return forest, self.oob_acc\n",
        "\n",
        "    def fit(self, df, no_trees=100):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "        # df_flitered : new df where columns containing only missing values have been dropped\n",
        "        # col_filter : list of the names of the remaining columns\n",
        "        df_filtered, col_filter = create_column_filter(dfcpy)\n",
        "        self.column_filter = col_filter\n",
        "\n",
        "        # df_imputed : new df missing numeric value in a column has been replaced by the mean of that column\n",
        "        # missing categoric value in a column has been replaced by the mode of that column imputation\n",
        "        # impt: mapping (dictionary) from column name to value that has replaced missing values\n",
        "        df_imputed, impt = create_imputation(df_filtered)\n",
        "        self.imputation = impt\n",
        "\n",
        "        #df_onehot: new df where each categoric feature has been replaced by a set of binary features\n",
        "        # one_hot: dictionary from column name to a set of categories (possible values for the feature)\n",
        "        # is the hot encoding right because also numerical values are here encoded?\n",
        "        df_onehot, one_h = create_one_hot(df_imputed)\n",
        "        self.one_hot = one_h\n",
        "\n",
        "       # list of class labels\n",
        "        self.labels = sorted(dfcpy[\"CLASS\"].unique())\n",
        "\n",
        "        onehot_labels = []\n",
        "        for label in dfcpy[\"CLASS\"]:\n",
        "            if label == 'positive':\n",
        "                onehot_labels.append(1)\n",
        "            else:\n",
        "                onehot_labels.append(0)\n",
        "        df_onehot['CLASS'] = onehot_labels\n",
        "\n",
        "        self.class_val = df_onehot['CLASS']\n",
        "        self.training_data = df_onehot\n",
        "        #print(self.training_data)\n",
        "\n",
        "        random_forest, oob = self.generate_forest(no_trees,df_onehot)\n",
        "        self.model = random_forest\n",
        "        self.oob = oob\n",
        "\n",
        "\n",
        "    def predict(self,df):\n",
        "        dfcpy = df.copy()\n",
        "\n",
        "\n",
        "        df_filtered = apply_column_filter(dfcpy,self.column_filter)\n",
        "        df_imputed = apply_imputation(df_filtered,self.imputation)\n",
        "        df_onehot = apply_one_hot(df_imputed,self.one_hot)\n",
        "        df_onehot = df_onehot.drop([\"CLASS\"],axis=1)\n",
        "\n",
        "        self.test_data = df_onehot.values\n",
        "\n",
        "        mapping = {}\n",
        "        for index, label in enumerate(self.labels):\n",
        "            mapping[label] = index\n",
        "\n",
        "        prediction_matrix = np.zeros((self.test_data.shape[0],len(self.labels)),dtype='float64')\n",
        "\n",
        "        no_trees = 0\n",
        "        for tree in self.model:\n",
        "            # Prediction of tree has class labels no of predictions\n",
        "            pred = tree.predict_proba(self.test_data)\n",
        "            #print(np.shape(pred))\n",
        "            prediction_matrix += pred\n",
        "            no_trees += 1\n",
        "\n",
        "        prediction_matrix /= no_trees\n",
        "        prediction_matrix = pd.DataFrame(prediction_matrix,columns=self.labels)\n",
        "\n",
        "        return prediction_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY5kQYI0M7cO",
        "outputId": "64101247-a6b8-4c2b-da36-c1b7fe18599d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 2.87 s.\n",
            "OOB accuracy: 0.9207\n",
            "Testing time: 0.07 s.\n",
            "Accuracy: 0.9102\n",
            "AUC: 0.9905\n",
            "Brier score: 0.1787\n"
          ]
        }
      ],
      "source": [
        "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
        "\n",
        "train_df = pd.read_csv(filepath+\"tic-tac-toe_train.csv\")\n",
        "\n",
        "test_df = pd.read_csv(filepath+\"tic-tac-toe_test.csv\")\n",
        "\n",
        "rf = RandomForest()\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rf.fit(train_df)\n",
        "print(\"Training time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"OOB accuracy: {:.4f}\".format(rf.oob_acc))\n",
        "test_labels = test_df[\"CLASS\"]\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "predictions = rf.predict(test_df)\n",
        "print(\"Testing time: {:.2f} s.\".format(time.perf_counter()-t0))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy(predictions,test_labels)))\n",
        "print(\"AUC: {:.4f}\".format(auc(predictions,test_labels))) # Comment this out if not implemented in assignment 1\n",
        "print(\"Brier score: {:.4f}\".format(brier_score(predictions,test_labels))) # Comment this out if not implemented in assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldJbxHeQOaP-",
        "outputId": "4d58b7c3-bb2b-46e7-9518-2d0dc8f0a076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 1.00\n",
            "AUC on training set: 1.00\n",
            "Brier score on training set: 0.02\n"
          ]
        }
      ],
      "source": [
        "train_labels = train_df[\"CLASS\"]\n",
        "rf = RandomForest()\n",
        "rf.fit(train_df)\n",
        "predictions = rf.predict(train_df)\n",
        "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
        "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
        "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "we2tVCOslhVw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}